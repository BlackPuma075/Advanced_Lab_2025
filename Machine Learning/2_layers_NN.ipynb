{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKwe7TLWwc6K"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GOvNe78lwkBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b55cae3-b984-48ef-d5e1-98be448d98bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Los datos corresponden a un subset del dataset cats vs dogs de https://www.kaggle.com/c/dogs-vs-cats\n",
        "files=glob.glob('/content/drive/MyDrive/Machine_Learning/CATS_DOGS/*.jpg')"
      ],
      "metadata": {
        "id": "dsUMuJTPwmij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 64\n",
        "\n",
        "train_data = np.array([\n",
        "    np.asarray(Image.open(file).resize((size, size)))\n",
        "    for file in files[:200]\n",
        "])\n",
        "\n",
        "train_label = np.array([\n",
        "    1 if 'dog' in file else 0\n",
        "    for file in files[:200]\n",
        "])\n",
        "\n",
        "test_label = np.array([\n",
        "    1 if 'dog' in file else 0\n",
        "    for file in files[200:]\n",
        "])\n",
        "\n",
        "test_data = np.array([\n",
        "    np.asarray(Image.open(file).resize((size, size)))\n",
        "    for file in files[200:]\n",
        "])"
      ],
      "metadata": {
        "id": "P15O8RgHwpO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_flatten = train_data.reshape(train_data.shape[0], -1).T\n",
        "train_set_label=train_label.reshape((1,train_label.shape[0]))\n",
        "#test_set_label=test_label.reshape((1,test_label.shape[0])) #Use test_label\n",
        "test_data_flatten = test_data.reshape(test_data.shape[0], -1).T"
      ],
      "metadata": {
        "id": "9hxyDblLwrHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "train_set_data= train_data_flatten/255\n",
        "test_set_data = test_data_flatten/np.max(test_data_flatten)"
      ],
      "metadata": {
        "id": "Lap2DwI9wsv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  a = 1/(1+np.exp(-z))\n",
        "  return a\n",
        "\n",
        "def tanh(z):\n",
        "  a = np.tanh(z)\n",
        "  return a\n",
        "\n",
        "def tanh_grad(x, y, iteraciones, alpha):\n",
        "  w = np.random.randn(x.shape[0], 1) * np.sqrt(1 / x.shape[0])\n",
        "  b = np.random.randn()\n",
        "  m = x.shape[1]\n",
        "  for i in range(iteraciones):\n",
        "    suma = 0\n",
        "    a = tanh(np.matmul(w.T,x)+b)\n",
        "    dz = (a - y) * (1 - a**2)\n",
        "    dw = (1/m) * np.dot(x, dz.T)\n",
        "    db = (1/m) * np.sum(dz)\n",
        "    w = w - alpha*dw\n",
        "    b = b - alpha*db\n",
        "    epsilon = 1e-8\n",
        "    a_clipped = np.clip(a, epsilon, 1 - epsilon)\n",
        "    J = -(1/m) * np.sum(y * np.log(a_clipped) + (1 - y) * np.log(1 - a_clipped))\n",
        "    f = i%(iteraciones/10)\n",
        "    if f==0:\n",
        "      print('J en la ',i,'ésima iteración: ',J)\n",
        "  return w, b, J\n",
        "\n",
        "def sigmoid_grad(x,y,w, b, iteraciones, alpha):\n",
        "  m = x.shape[1]\n",
        "  #w,b = np.zeros((len(train_set_data),1)),0 #Inicializamos los pesos en cero\n",
        "  for i in range(iteraciones):\n",
        "    suma = 0 #Defino la variable donde guardaré el valor de la sumatoria para el cálculo del costo\n",
        "    a = sigmoid(np.matmul(w,x)+b.reshape(-1, 1)) #Calcula la función de activación con w,b iguales a cero en principio\n",
        "    dw = (1/m) * np.matmul((a - y), x.T)  # dw: (3, 12288)\n",
        "    w = w - alpha * dw\n",
        "    db = np.mean(a - y)\n",
        "    b = b - alpha * db #Actualizamos b usando db\n",
        "    #b = b-alpha*(1/m)*(a-y) #Actualizamos b usando db\n",
        "    for j in range(a.shape[1]): #Hacemos el cálculo del costo con el w,b iniciales\n",
        "      suma += y[:1,j]*np.log(a[:1,j])+(1-y[:1,j])*np.log(1-(a[:1,j]))\n",
        "    J = -(1/m)*sum(suma) #Calculamos el costo\n",
        "    f = i%(iteraciones/10)\n",
        "    if f == 0:\n",
        "      print('J en la ',i,'-ésima iteracion:', J) #Imprimimos el costo en cada iteración\n",
        "  return w, b, J\n",
        "\n",
        "def binary(x, y, w, b):\n",
        "  ytst = sigmoid(np.matmul(w, x)+b.reshape(-1,1))\n",
        "  return ytst\n",
        "\n",
        "def precission(data, labels):\n",
        "  y_test = np.zeros_like(data)\n",
        "  for j in range(3):\n",
        "    for i in range(len(data[0].shape)):\n",
        "      if data[j, i] > 0.5:\n",
        "        y_test[j,i]=1\n",
        "      else:\n",
        "        y_test[j,i]=0\n",
        "\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  y_pred = np.argmax(y_test, axis=0)\n",
        "  accuracy = accuracy_score(y_pred, labels)\n",
        "  print('La precisión es del ',accuracy,'%')\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "def NN(x_train, y_train, x_test, y_test, iteraciones, alpha):\n",
        "    w_list = []\n",
        "    b_list = []\n",
        "    for i in range(3):  # Número de neuronas\n",
        "        omega, bias, J = tanh_grad(x_train, y_train, iteraciones, alpha)\n",
        "        w_list.append(omega)\n",
        "        b_list.append(bias)\n",
        "\n",
        "    w_array = np.array(w_list)  # Forma: (3, 12288, 1)\n",
        "    b_array = np.array(b_list)  # Forma: (3,)\n",
        "\n",
        "    w, b, J = sigmoid_grad(x_train, y_train,w_array[:,:,0], b_array, iteraciones, alpha)\n",
        "\n",
        "    precision_test = binary(x_test, y_test, w, b)\n",
        "\n",
        "    accuracy = precission(precision_test, y_test)\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "MKG-DZRlwwPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def initialize_parameters(n_x, n_h):\n",
        "    W1 = np.random.randn(n_h, n_x) * np.sqrt(1. / n_x)\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(1, n_h) * np.sqrt(1. / n_h)\n",
        "    b2 = np.zeros((1, 1))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tanh(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    return A1, A2\n",
        "\n",
        "def compute_cost(A2, Y):\n",
        "    m = Y.shape[1]\n",
        "    epsilon = 1e-8\n",
        "    A2 = np.clip(A2, epsilon, 1 - epsilon)\n",
        "    return -1/m * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n",
        "\n",
        "def backward_propagation(X, Y, A1, A2, W2):\n",
        "    m = X.shape[1]\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = dA1 * (1 - A1**2)\n",
        "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
        "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 -= alpha * dW1\n",
        "    b1 -= alpha * db1\n",
        "    W2 -= alpha * dW2\n",
        "    b2 -= alpha * db2\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def predict(A2):\n",
        "    return (A2 > 0.5).astype(int)\n",
        "\n",
        "def accuracy(Y_pred, Y_true):\n",
        "    return accuracy_score(Y_true.flatten(), Y_pred.flatten())\n",
        "\n",
        "def neural_network(X, Y, X_test, Y_test, n_h, iterations, alpha):\n",
        "    n_x = X.shape[0]\n",
        "    W1, b1, W2, b2 = initialize_parameters(n_x, n_h)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
        "        cost = compute_cost(A2, Y)\n",
        "        dW1, db1, dW2, db2 = backward_propagation(X, Y, A1, A2, W2)\n",
        "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "\n",
        "        if i % (iterations // 10) == 0:\n",
        "            print(f'Costo en la iteración {i}: {cost:.4f}')\n",
        "\n",
        "    _, A2_test = forward_propagation(X_test, W1, b1, W2, b2)\n",
        "    Y_pred = predict(A2_test)\n",
        "    acc = accuracy(Y_pred, Y_test)\n",
        "\n",
        "    print(f'Precisión final en test: {acc:.4f}')\n",
        "    return W1, b1, W2, b2, acc\n"
      ],
      "metadata": {
        "id": "1IP_pJHlayTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1, W2, b2, acc = neural_network(train_set_data, train_set_label,test_set_data,test_label, n_h=3, iterations=10000, alpha=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhZvZLpca0wo",
        "outputId": "6ebb85e6-fe55-4570-b2d9-1eafe978db0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Costo en la iteración 0: 0.7041\n",
            "Costo en la iteración 1000: 0.1919\n",
            "Costo en la iteración 2000: 0.0577\n",
            "Costo en la iteración 3000: 0.0302\n",
            "Costo en la iteración 4000: 0.0205\n",
            "Costo en la iteración 5000: 0.0155\n",
            "Costo en la iteración 6000: 0.0124\n",
            "Costo en la iteración 7000: 0.0103\n",
            "Costo en la iteración 8000: 0.0089\n",
            "Costo en la iteración 9000: 0.0078\n",
            "Precisión final en test: 0.5213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w, b = NN(train_set_data, train_set_label,test_set_data,test_label,  1000, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td7thQsDC8vY",
        "outputId": "aeea9418-4e81-40ab-973f-bf7fa2639bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J en la  0 ésima iteración:  11.236615257710946\n",
            "J en la  100 ésima iteración:  7.043000882559377\n",
            "J en la  200 ésima iteración:  7.04287202396657\n",
            "J en la  300 ésima iteración:  7.04274282907563\n",
            "J en la  400 ésima iteración:  7.042613296098996\n",
            "J en la  500 ésima iteración:  7.042483423301193\n",
            "J en la  600 ésima iteración:  7.0423532088833385\n",
            "J en la  700 ésima iteración:  7.042222651016021\n",
            "J en la  800 ésima iteración:  7.042091747880553\n",
            "J en la  900 ésima iteración:  7.041960497694228\n",
            "J en la  0 ésima iteración:  0.7531965910655696\n",
            "J en la  100 ésima iteración:  7.161712231600566\n",
            "J en la  200 ésima iteración:  7.161712179788638\n",
            "J en la  300 ésima iteración:  7.161712127975977\n",
            "J en la  400 ésima iteración:  7.161712076163219\n",
            "J en la  500 ésima iteración:  7.161712024349726\n",
            "J en la  600 ésima iteración:  7.1617119725355005\n",
            "J en la  700 ésima iteración:  7.161711920720543\n",
            "J en la  800 ésima iteración:  7.1617118689054875\n",
            "J en la  900 ésima iteración:  7.161711817089697\n",
            "J en la  0 ésima iteración:  9.026286154482028\n",
            "J en la  100 ésima iteración:  7.159264410445598\n",
            "J en la  200 ésima iteración:  7.15926432595674\n",
            "J en la  300 ésima iteración:  7.1592642414661976\n",
            "J en la  400 ésima iteración:  7.159264156973968\n",
            "J en la  500 ésima iteración:  7.159264072480442\n",
            "J en la  600 ésima iteración:  7.1592639879856215\n",
            "J en la  700 ésima iteración:  7.159263903489503\n",
            "J en la  800 ésima iteración:  7.159263818991699\n",
            "J en la  900 ésima iteración:  7.159263734492599\n",
            "J en la  0 -ésima iteracion: 5.121243141685125\n",
            "J en la  100 -ésima iteracion: 3.4577905262068866\n",
            "J en la  200 -ésima iteracion: 0.5693771038951946\n",
            "J en la  300 -ésima iteracion: 0.5678563343435922\n",
            "J en la  400 -ésima iteracion: 1.0912177842533002\n",
            "J en la  500 -ésima iteracion: 1.0530966192068487\n",
            "J en la  600 -ésima iteracion: 0.710728998231336\n",
            "J en la  700 -ésima iteracion: 0.12721637228035237\n",
            "J en la  800 -ésima iteracion: 0.10775044345796482\n",
            "J en la  900 -ésima iteracion: 0.09756687580786208\n",
            "La precisión es del  0.505256648113791 %\n"
          ]
        }
      ]
    }
  ]
}